import streamlit as st
import re
import pandas as pd
from collections import defaultdict
from datetime import date 
import io 
from urllib.parse import quote 

# N√ÅZOV P√îVODN√âHO KARTAL√ìGVO√âHO S√öBORU
CATALOG_FILENAME = "ES Katalog biotopov Suvada ed 2023 v1.05.txt"

# --- CORE FACTORY AND DATA FUNCTIONS (Pre-cached) ---

def inner_dict_factory():
    """Pou≈æ√≠va sa ako factory pre vnoren√Ω defaultdict namiesto nesp√¥sobnej lambda funkcie."""
    return defaultdict(int)

@st.cache_data
def load_file_content(filename):
    """Naƒç√≠ta obsah katal√≥gu zo s√∫boru, sk√∫≈°a be≈æn√© k√≥dovania."""
    try:
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            with open(filename, 'r', encoding='Windows-1250') as f:
                return f.read()
    except FileNotFoundError:
        st.error(f"‚ö†Ô∏è CHYBA: S√∫bor s d√°tami '{filename}' sa nena≈°iel v prieƒçinku aplik√°cie.")
        st.caption("Uistite sa, ≈æe s√∫bor m√° presne tento n√°zov a je v rovnakom prieƒçinku ako python skript.")
        return None
    except Exception as e:
        st.error(f"Chyba pri naƒç√≠tan√≠ s√∫boru: {e}")
        return None

@st.cache_data
def parse_catalog_data(catalog_text):
    """Spracuje text katal√≥gu a extrahuje mapu synon√Ωm (Sekcia 1) a maticu podobnosti (Sekcia 4)."""
    
    lines = catalog_text.split('\n')
    section_1_active = False
    section_4_active = False
    
    synonym_map = {}
    similarity_matrix = defaultdict(inner_dict_factory)
    group_names = {}
    current_canonical_name = None
    
    # Regul√°rne v√Ωrazy 
    re_section_1_start = re.compile(r"SECTION 1:\s*Species aggregation", re.IGNORECASE)
    re_section_4_start = re.compile(r"SECTION 4:\s*Similarity", re.IGNORECASE) 
    re_section_end = re.compile(r"SECTION [23]:", re.IGNORECASE)
    re_canonical_name_1 = re.compile(r"^([A-Za-z].*?)\s+-\s*(\d+)\s*$")
    re_species_entry_1 = re.compile(r"^\s+([A-Za-z].*?)\s+(\d+)\s*$")
    re_group_name_4 = re.compile(r"^(Group\d+)\s*name:\s*(.+)\s*$") 
    re_species_name_only = re.compile(r"^\s*([A-Za-z].+?)\s*$", re.IGNORECASE) 
    re_total_line = re.compile(r"^\s*Total:\s*(\d+)\s*$", re.IGNORECASE)
    re_matrix_entry_4 = re.compile(r"^\s*(Group\d+):\s*(\d+)\s*$", re.IGNORECASE)
    
    current_species_in_matrix = None
    group_names_found = 0
    matrix_entries_found = 0

    for line in lines:
        line_clean = line.strip()

        if re_section_1_start.search(line):
            section_1_active = True; section_4_active = False; continue
        elif re_section_4_start.search(line):
            section_1_active = False; section_4_active = True; current_canonical_name = None; continue
        elif re_section_end.search(line):
            section_1_active = False; section_4_active = False; continue
        
        if section_1_active:
            match_canonical = re_canonical_name_1.match(line_clean)
            if match_canonical:
                current_canonical_name = match_canonical.group(1).strip()
                continue
            match_synonym = re_species_entry_1.match(line) 
            if match_synonym and current_canonical_name:
                synonym = match_synonym.group(1).strip()
                if synonym not in synonym_map: synonym_map[synonym] = current_canonical_name
            
        elif section_4_active:
            match_group_name = re_group_name_4.match(line_clean)
            if match_group_name:
                group_id = match_group_name.group(1).strip()
                # Uklad√°me pln√Ω n√°zov, ktor√Ω obsahuje aj k√≥d biotypu
                group_name_full = match_group_name.group(2).split(" Count:")[0].strip()
                group_names[group_id] = group_name_full
                group_names_found += 1
                continue
            
            if line_clean.startswith("Count:") or line_clean.startswith("No.") or line_clean.startswith("Frequency table"):
                continue

            match_species_line = re_species_name_only.match(line)
            if match_species_line and 'Total:' not in line and 'Group' not in line:
                current_species_in_matrix = match_species_line.group(1).strip()
                continue
            
            if re_total_line.match(line): continue
            
            match_matrix_entry = re_matrix_entry_4.match(line)
            if match_matrix_entry and current_species_in_matrix:
                group_id = match_matrix_entry.group(1).strip()
                try:
                    count = int(match_matrix_entry.group(2))
                    similarity_matrix[current_species_in_matrix][group_id] = count
                    matrix_entries_found += 1
                except ValueError: pass
                
    if not group_names_found or not matrix_entries_found:
        return None, None, None
         
    return synonym_map, group_names, similarity_matrix

@st.cache_data
def calculate_total_frequency_per_group(similarity_matrix, group_names):
    """Vypoƒç√≠ta s√∫ƒçet frekvenci√≠ pre V≈†ETKY kanonick√© druhy pre ka≈æd√Ω biotyp (Max Score)."""
    total_frequency = defaultdict(int)
    all_groups = set(group_names.keys())

    for canonical_name in similarity_matrix:
        species_data = similarity_matrix[canonical_name]
        for group_id, count in species_data.items():
            if group_id in all_groups:
                total_frequency[group_id] += count
    
    return dict(total_frequency)


def get_canonical_name(species_name, synonym_map):
    """Z√≠ska kanonick√© meno druhu, ak existuje, inak vr√°ti p√¥vodn√© meno."""
    species_name = species_name.strip()
    return synonym_map.get(species_name, species_name)

@st.cache_data
def get_all_known_species(synonym_map, similarity_matrix):
    """Z√≠ska zjednoten√Ω zoznam v≈°etk√Ωch zn√°mych druhov a synon√Ωm."""
    canonical_species = set(similarity_matrix.keys())
    all_known = canonical_species.union(set(synonym_map.keys())).union(set(synonym_map.values()))
    return sorted(list(all_known))

# --- ANALYTICK√Å FUNKCIA S FQI V√ùPOƒåTOM ---

@st.cache_data(show_spinner="Prebieha v√Ωpoƒçet Frekvenƒçn√©ho Indexu (FQI)...")
def analyze_similarity(species_list, synonym_map, group_names, similarity_matrix, total_frequency_per_group):
    """
    Vyhodnot√≠ podobnos≈• k biotopom (skupin√°m).
    FQI = (Kumulat√≠vne sk√≥re zadan√Ωch druhov / Celkov√© mo≈æn√© sk√≥re skupiny) * 100
    Tie≈æ sleduje, ktor√© vstupy boli preskoƒçen√© (kanonick√Ω duplik√°t).
    """
    
    cumulative_scores = defaultdict(int)
    valid_groups = set(group_names.keys())
    processed_canonical_species = set() 
    name_conversion_map = {} 
    ignored_inputs = [] 
    
    # 1. KUMULAT√çVNE Sƒå√çTANIE A KONVERZIA
    for user_species in species_list:
        user_species = user_species.strip()
        canonical_name = get_canonical_name(user_species, synonym_map)
        
        if canonical_name in similarity_matrix:
            
            name_conversion_map[user_species] = canonical_name

            if canonical_name not in processed_canonical_species:
                processed_canonical_species.add(canonical_name)
                
                species_data = similarity_matrix[canonical_name]
                for group_id, count in species_data.items():
                    if group_id in valid_groups:
                        cumulative_scores[group_id] += count
            else:
                # Kanonick√Ω druh bol u≈æ spracovan√Ω, tento vstup ignorujeme
                ignored_inputs.append(user_species)
                

    if not cumulative_scores:
        return None, processed_canonical_species, name_conversion_map, ignored_inputs 

    # 2. V√ùPOƒåET FQI (Percentu√°lna normaliz√°cia)
    fqi_scores = {}
    
    for group_id, cumulative_score in cumulative_scores.items():
        max_score = total_frequency_per_group.get(group_id, 0)
        
        if max_score > 0:
            fqi = (cumulative_score / max_score) * 100
            fqi_scores[group_id] = fqi
        else:
            fqi_scores[group_id] = 0.0

    # 3. ZORADENIE A V√ùBER TOP 3
    sorted_scores = sorted(fqi_scores.items(), key=lambda item: item[1], reverse=True)
    top_matches_data = []
    
    # Regex pre robustn√∫ extrakciu k√≥du (prv√Ω non-whitespace token) a zvy≈°ku n√°zvu
    re_biotope_code_extractor = re.compile(r'^(\S+)\s+(.*)', re.IGNORECASE)

    for rank, (group_id, score) in enumerate(sorted_scores[:3]):
        biotope_full_name = group_names.get(group_id, f"Nezn√°my Biotop ({group_id})")
        
        # P√¥vodn√Ω group_id (napr. Group42) ako fallback
        biotope_code = group_id 
        biotope_name = biotope_full_name

        match_code = re_biotope_code_extractor.match(biotope_full_name)
        
        if match_code:
            # Ak regex n√°jde zhodu
            biotope_code = match_code.group(1).strip() 
            biotope_name = match_code.group(2).strip()
            
            # Odstr√°nenie voliteƒænej pomlƒçky/medzier na zaƒçiatku n√°zvu, ak tam zostala
            if biotope_name.startswith('-'):
                 biotope_name = biotope_name[1:].strip()

        
        top_matches_data.append({
            'Poradie': rank + 1,
            'K√ìD Biotopu': biotope_code, # Zobrazi≈• skratku LES05.1a, TRB01a atƒè.
            'N√°zov Biotopu': biotope_name, # Zobrazi≈• pln√Ω n√°zov
            'FQI (% Zhody)': f"{score:.2f} %", 
        })

    return top_matches_data, processed_canonical_species, name_conversion_map, ignored_inputs

# --- EXPORTN√Å FUNKCIA PRE TXT ---

def generate_export_data(fqi_results_df, canonical_species_list, manual_data):
    """
    Generuje ucelen√Ω textov√Ω re≈•azec pre export obsahuj√∫ci hlaviƒçku, FQI v√Ωsledky a zoznam druhov.
    """
    
    # Doln√© indexy pre et√°≈æe
    E3, E2, E1, E0 = "\u2083", "\u2082", "\u2081", "\u2080"
    
    # Prevod DataFrame na textov√∫ tabuƒæku (CSV s tabul√°torom pre ƒçitateƒænos≈•)
    fqi_table = fqi_results_df.reset_index(drop=True).to_csv(sep='\t', index=False)
    
    output = "--- EXPORT V√ùSLEDKOV ANAL√ùZY BIOTOPU ---\n\n"
    
    # 1. HLAVIƒåKA PRE MANU√ÅLNY Z√ÅPIS
    output += "SEKCIA 1: √öDAJE Z TER√âNU (VYPLNEN√â V APLIK√ÅCII)\n"
    output += "--------------------------------------------------\n"
    output += f"Lokalita:              {manual_data['lokalita']}\n"
    output += f"S√∫radnice:             {manual_data['suradnica']}\n"
    output += f"Meno mapovateƒæa:       {manual_data['mapovatel']}\n"
    output += f"D√°tum:                 {manual_data['datum'].strftime('%Y-%m-%d') if isinstance(manual_data['datum'], date) else manual_data['datum']}\n"
    output += f"Pokryvnos≈• et√°≈æ√≠ (E{E3}: stromov√©, E{E2}: krovit√©, E{E1}: bylinn√©, E{E0}: machov√©/li≈°ajn√≠kov√©):\n"
    output += f"  E{E3}:                  {manual_data['pokryvnost_E3']}\n"
    output += f"  E{E2}:                  {manual_data['pokryvnost_E2']}\n"
    output += f"  E{E1}:                  {manual_data['pokryvnost_E1']}\n"
    output += f"  E{E0}:                  {manual_data['pokryvnost_E0']}\n\n"
    
    # 2. V√ùSLEDKY FQI ANAL√ùZY
    output += "SEKCIA 2: V√ùSLEDKY FQI ANAL√ùZY (TOP 3)\n"
    output += "--------------------------------------------------\n"
    output += fqi_table
    output += "\n"

    # 3. KANONICK√â DRUHY
    output += "SEKCIA 3: POU≈ΩIT√â KANONICK√â DRUHY\n"
    output += "--------------------------------------------------\n"
    output += "Poƒçet kanonick√Ωch druhov: " + str(len(canonical_species_list)) + "\n"
    output += "\n".join(sorted(canonical_species_list))
    output += "\n\n--- KONIEC EXPORTU ---\n"
    
    return output

# --- EXPORTN√Å FUNKCIA PRE XLSX ---

def generate_excel_data(fqi_results_df, canonical_species_list, manual_data):
    """Generuje Excel s√∫bor (.xlsx) s tromi listami d√°t."""
    
    # Doln√© indexy pre et√°≈æe
    E3, E2, E1, E0 = "\u2083", "\u2082", "\u2081", "\u2080"
    
    # 1. PRIPRAVA DAT PRE HLAVICKU (ako DataFrame)
    header_data = [
        ("--- Z√ÅKLADN√â √öDAJE ---", ""),
        ("Lokalita", manual_data['lokalita']),
        ("S√∫radnice", manual_data['suradnica']),
        ("Meno mapovateƒæa", manual_data['mapovatel']),
        ("D√°tum", manual_data['datum'].strftime('%Y-%m-%d') if isinstance(manual_data['datum'], date) else manual_data['datum']),
        ("--- POKRYVNOS≈§ ET√Å≈Ω√ç ---", ""),
        (f"E{E3} (Stromov√© poschodie)", manual_data['pokryvnost_E3']),
        (f"E{E2} (Krovit√© poschodie)", manual_data['pokryvnost_E2']),
        (f"E{E1} (Bylinn√© poschodie)", manual_data['pokryvnost_E1']),
        (f"E{E0} (Machov√©/Li≈°. poschodie)", manual_data['pokryvnost_E0']),
    ]
    df_header = pd.DataFrame(header_data, columns=['Popis', 'Hodnota'])
    
    # 2. PRIPRAVA DAT PRE DRUHY
    df_species = pd.DataFrame(sorted(canonical_species_list), columns=['Kanonick√© druhy (pou≈æit√© v anal√Ωze)'])

    # 3. ZAPIS DO BYTESIO BUFFERU
    output = io.BytesIO()
    
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        
        # A. Manu√°lne √∫daje (Hlaviƒçka)
        df_header.to_excel(writer, sheet_name='Data z ter√©nu', index=False, startrow=0, startcol=0)

        # B. FQI V√Ωsledky (u≈æ je DataFrame)
        df_fqi_excel = fqi_results_df.copy()
        df_fqi_excel.to_excel(writer, sheet_name='FQI V√Ωsledky', index=False, startrow=0, startcol=0)

        # C. Kanonick√© druhy
        df_species.to_excel(writer, sheet_name='Kanonick√© druhy', index=False, startrow=0, startcol=0)

        # Optimaliz√°cia ≈°√≠rky stƒ∫pcov pre lep≈°iu ƒçitateƒænos≈•
        for sheetname in writer.sheets:
            worksheet = writer.sheets[sheetname]
            # Nastav√≠ ≈°√≠rku pre prv√© 2 stƒ∫pce
            worksheet.set_column('A:D', 30)
            
    # Resetovanie poz√≠cie bufferu a vr√°tenie obsahu
    output.seek(0)
    return output.read()

# --- AKCIE PRE TLAƒåIDL√Å (Callbacks) ---

def calculate_fqi_action():
    """Ulo≈æ√≠ aktu√°lny v√Ωber a prepne re≈æim na zobrazenie v√Ωsledkov."""
    st.session_state['calculated_species'] = st.session_state.selected_species_multiselect
    st.session_state['app_mode'] = 'results'

def reset_selection_action():
    """Prepne re≈æim sp√§≈• na v√Ωber."""
    st.session_state['app_mode'] = 'selection'


# --- HLAVN√Å WEB APLIK√ÅCIA ---

def biotope_web_app():
    
    st.set_page_config(page_title="Identifik√°tor Biotopov (FQI)", layout="wide")
    
    st.title("üåø Identifik√°tor Biotopov (FQI) na z√°klade Expertn√©ho Syst√©mu")
    st.caption(f"D√°ta naƒç√≠tan√© zo s√∫boru: **{CATALOG_FILENAME}**")

    # Cit√°cia
    st.markdown("""
        **Podƒæa publik√°cie:**
        ≈†uvada R. (ed.), 2023: Katal√≥g biotopov Slovenska. Druh√©, roz≈°√≠ren√© vydanie. ‚Äì
        ≈†t√°tna ochrana pr√≠rody SR, Bansk√° Bystrica, 511 p. ISBN 978-80-8184-106-4
    """)
    st.markdown("---")


    # Inicializ√°cia stavu
    if 'app_mode' not in st.session_state:
        st.session_state['app_mode'] = 'selection'
        st.session_state['calculated_species'] = [] 

    # Krok 0: Naƒç√≠tanie a parsovanie d√°t (Cache d√°ta)
    catalog_text = load_file_content(CATALOG_FILENAME)
    if catalog_text is None:
        return

    synonym_map, group_names, similarity_matrix = parse_catalog_data(catalog_text)
    if synonym_map is None: 
        st.error("Nepodarilo sa spracova≈• d√°ta z katal√≥gu. Skontrolujte jeho form√°tovanie.")
        return
        
    all_species = get_all_known_species(synonym_map, similarity_matrix)
    total_frequency_per_group = calculate_total_frequency_per_group(similarity_matrix, group_names)

    # Sidebar ≈°tatistiky
    st.sidebar.header("≈†tatistiky D√°t")
    st.sidebar.write(f"Biotopov (skup√≠n): **{len(group_names)}**")
    st.sidebar.write(f"Spracovan√Ωch druhov v matici: **{len(similarity_matrix)}**")
    st.sidebar.write(f"Celkov√Ω poƒçet n√°zvov/synon√Ωm na v√Ωber: **{len(all_species)}**")


    # --- RIADENIE RE≈ΩIMU APLIK√ÅCIE ---

    if st.session_state['app_mode'] == 'selection':
        # Re≈æim 1: V√ùBER DRUHOV

        st.header("1. V√Ωber Druhov")
        st.warning("V√Ωpoƒçet FQI sa spust√≠ a≈æ po stlaƒçen√≠ tlaƒçidla 'V≈°etky druhy zadan√©, vypoƒç√≠taj' pod zoznamom.")
        
        current_species_list = st.multiselect(
            "Vyberte druh zo zoznamu (zaƒçnite p√≠sa≈• pre filtrovanie):",
            options=all_species,
            default=st.session_state['calculated_species'], 
            key="selected_species_multiselect" 
        )
        
        st.info(f"Aktu√°lne vybran√Ωch druhov: **{len(current_species_list)}**")
        
        if current_species_list:
            st.button(
                "üü¢ V≈°etky druhy zadan√©, vypoƒç√≠taj", 
                on_click=calculate_fqi_action, 
                use_container_width=True
            )
        else:
            st.button("V≈°etky druhy zadan√©, vypoƒç√≠taj", disabled=True, use_container_width=True)


    elif st.session_state['app_mode'] == 'results':
        # Re≈æim 2: ZOBRAZENIE V√ùSLEDKOV

        user_species_list = st.session_state['calculated_species']

        if not user_species_list:
            st.error("Chyba: Neboli n√°jden√© ≈æiadne druhy na anal√Ωzu. Prepnite sp√§≈• na v√Ωber.")
            st.button("‚¨ÖÔ∏è Zme≈à druhov√∫ skupinu", on_click=reset_selection_action)
            return

        st.header("2. V√Ωsledky Anal√Ωzy FQI")
        
        st.button("‚¨ÖÔ∏è Zme≈à druhov√∫ skupinu", on_click=reset_selection_action)
        
        st.markdown("---")
        
        st.info(f"Anal√Ωza be≈æ√≠ pre **{len(user_species_list)}** vybran√Ωch druhov.")

        # Spustenie FQI anal√Ωzy (cache)
        top_matches_data, processed_species, name_conversion_map, ignored_inputs = analyze_similarity(
            user_species_list, synonym_map, group_names, similarity_matrix, total_frequency_per_group
        )
        
        if top_matches_data is None:
            st.error("Nena≈°iel sa ≈æiaden zadan√Ω druh v matici podobnosti. V√Ωpoƒçet FQI nie je mo≈æn√Ω.")
            return

        # Krok 3: Zobrazenie v√Ωsledkov
        
        # 3.1. TOP 3 ZHODY
        st.subheader("Biotopy s najvy≈°≈°ou podobnos≈•ou (FQI)")
        
        df_results = pd.DataFrame(top_matches_data)
        df_results_display = df_results.set_index('Poradie')
        st.dataframe(df_results_display, use_container_width=True)

        st.caption("FQI (Frekvenƒçn√Ω Index) je **%**, ktor√© vyjadruje podiel s√∫ƒçtu frekvenci√≠ vybran√Ωch druhov na celkovej mo≈ænej frekvencii v≈°etk√Ωch kanonick√Ωch druhov v danej skupine. Vy≈°≈°ie percento = Vy≈°≈°ia zhoda.")

        st.markdown("---")
        
        # --- NOV√Å SEKCIA 3: DETAIY SPRACOVANIA (P√¥vodne 5) ---
        st.subheader("3. Detaily Spracovania")

        col1, col2, col3 = st.columns(3) 

        with col1:
            st.markdown("##### Spracovan√© druhy (kanonick√©)")
            st.write(f"**Poƒçet spracovan√Ωch kanonick√Ωch druhov:** {len(processed_species)}")
            
            with st.expander("Zobrazi≈• pou≈æit√© kanonick√© men√°"):
                st.code("\n".join(processed_species))

        with col2:
            conversions = {original: canonical for original, canonical in name_conversion_map.items() if original != canonical}
            
            st.markdown(f"##### Konverzie Synonym (zadan√Ω ‚Üí kanonick√Ω)")
            
            if conversions:
                df_conversions = pd.DataFrame(list(conversions.items()), columns=['Zadan√© meno', 'Kanonick√© meno'])
                st.dataframe(df_conversions, use_container_width=True, hide_index=True)
            else:
                st.success("Neboli zadan√© ≈æiadne synonym√°.")

        with col3:
            st.markdown(f"##### Ignorovan√© duplik√°ty vstupu")
            
            if ignored_inputs:
                st.warning(f"**Ignorovan√Ωch vstupov: {len(ignored_inputs)}**")
                st.caption("Tieto druhy maj√∫ kanonick√© meno, ktor√© u≈æ bolo v r√°mci v√Ωpoƒçtu zahrnut√©. Boli preskoƒçen√©, aby sa predi≈°lo duplicitn√©mu zapoƒç√≠taniu.")
                with st.expander("Zobrazi≈• ignorovan√© vstupy"):
                    st.code("\n".join(ignored_inputs))
            else:
                st.success("Neboli zadan√© ≈æiadne duplik√°ty (synonym√° ani kanonick√© men√°) k rovnak√©mu kanonick√©mu druhu.")

        st.markdown("---") 

        # --- NOV√Å SEKCIA 4: √öDAJE Z TER√âNU A EXPORT (P√¥vodne 3.2) ---
        st.subheader("4. √ödaje z ter√©nu a Export")
        
        # Pou≈æitie doln√Ωch indexov
        E3, E2, E1, E0 = "\u2083", "\u2082", "\u2081", "\u2080"
        
        # Uchovanie d√°t zadan√Ωch do formul√°ru pre export
        lokalita, suradnica, mapovatel, datum = "", "", "", date.today()
        pokryvnost_E3, pokryvnost_E2, pokryvnost_E1, pokryvnost_E0 = "0", "0", "0", "0"

        with st.form("field_data_form"):
            
            # ZMENA: Nastavenie pomeru stƒ∫pcov na 3:1 (pre z√∫≈æenie col_b)
            col_a, col_b = st.columns([3, 1]) 
            
            # --- ZAD√ÅVANIE √öDAJOV Z TER√âNU (≈†ir≈°√≠ stƒ∫pec) ---
            with col_a:
                # NOV√ù NADPIS (TERAZ VN√öTRI STƒπPCA)
                st.markdown("##### Inform√°cie o ter√©nnom z√°zname")
                
                lokalita = st.text_input("Lokalita", key='export_lokalita')
                suradnica = st.text_input("S√∫radnice", key='export_suradnica')
                mapovatel = st.text_input("Meno mapovateƒæa", key='export_mapovatel')
                datum = st.date_input("D√°tum z√°pisu", value=date.today(), key='export_datum')

            # --- POKRYVNOS≈§ ET√Å≈Ω√ç (U≈æ≈°√≠ stƒ∫pec 1:3) ---
            with col_b:
                # Titulok stƒ∫pca je teraz zarovno s titulkom v col_a
                st.markdown(f"##### Pokryvnos≈• et√°≈æ√≠ (E{E3}-E{E0})")
                
                # Zjednodu≈°en√° n√°poveda
                help_text_etaze = "Pokryvnos≈• v %"
                # P√¥vodn√© dlh≈°ie labely - polia sa z√∫≈æia vƒèaka √∫zkemu stƒ∫pcu
                pokryvnost_E3 = st.text_input(f"E{E3} (Stromov√© poschodie)", value="", key='export_E3', help=help_text_etaze)
                pokryvnost_E2 = st.text_input(f"E{E2} (Krovit√© poschodie)", value="", key='export_E2', help=help_text_etaze)
                pokryvnost_E1 = st.text_input(f"E{E1} (Bylinn√© poschodie)", value="", key='export_E1', help=help_text_etaze)
                pokryvnost_E0 = st.text_input(f"E{E0} (Machov√©/Li≈°. poschodie)", value="", key='export_E0', help=help_text_etaze)
                
            st.form_submit_button("Ulo≈æi≈• √∫daje (pred exportom)", type="primary")

        # Zostavenie manu√°lnych d√°t pre export
        manual_data = {
            'lokalita': lokalita,
            'suradnica': suradnica,
            'mapovatel': mapovatel,
            'datum': datum,
            'pokryvnost_E3': pokryvnost_E3,
            'pokryvnost_E2': pokryvnost_E2,
            'pokryvnost_E1': pokryvnost_E1,
            'pokryvnost_E0': pokryvnost_E0,
        }

        # Generovanie obsahu pre TXT export
        export_data_str = generate_export_data(
            df_results, 
            list(processed_species), 
            manual_data
        )
        
        # Generovanie obsahu pre XLSX export
        excel_data_bytes = generate_excel_data(
            df_results, 
            list(processed_species), 
            manual_data
        )
        
        # Tlaƒçidl√° pre stiahnutie v stƒ∫pcoch (vyrovnan√© na jednom riadku)
        file_name_prefix = lokalita[:10].replace(' ', '_').strip() if lokalita else "novy_zapis"
        
        col_xlsx, col_txt = st.columns(2)
        
        with col_xlsx: 
            st.download_button(
                label="‚¨áÔ∏è Export v√Ωsledkov (Excel XLSX)",
                data=excel_data_bytes,
                file_name=f"biotop_analyza_{date.today().strftime('%Y%m%d')}_{file_name_prefix}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )

        with col_txt: 
            st.download_button(
                label="‚¨áÔ∏è Export v√Ωsledkov (TXT form√°t)",
                data=export_data_str,
                file_name=f"biotop_analyza_{date.today().strftime('%Y%m%d')}_{file_name_prefix}.txt",
                mime="text/plain",
                use_container_width=True
            )
            
        st.markdown("---") 
            

    # Copyright Footer
    st.markdown("---")
    st.markdown("<footer><p style='text-align: right; color: gray; font-size: small;'>¬© R√≥bert ≈†uvada 2025</p></footer>", unsafe_allow_html=True)


if __name__ == "__main__":
    biotope_web_app()